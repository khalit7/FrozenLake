Question 2 : How many iterations did policy iteration require to find an optimal policy for the big frozen lake? How many iterations did value iteration require? Which algorithm was faster?

**********************policy iteration took 6 iterations to find the optimal policy. The algorithm ran in 0.15483480000000005 ms********************************

**********************value iteration took 20 iterations to find the optimal policy. The algorithms ran in 0.0634827 ms*****************************************

Question3: How many episodes did Sarsa control require to find an optimal policy for the small frozen lake? How many episodes did Q-learning control require?

## Sarsa
****************************** Sarsa converaged on episode 11042 **********************
Lake:
[['&' '.' '.' '.']
 ['.' '#' '.' '#']
 ['.' '.' '.' '#']
 ['#' '.' '.' '$']]
Policy:
[['↓' '→' '↓' '←']
 ['↓' '↑' '↓' '↑']
 ['→' '→' '↓' '↑']
 ['↑' '→' '→' '↑']]
Value:
[[0.371 0.41  0.496 0.409]
 [0.452 0.    0.639 0.   ]
 [0.512 0.601 0.774 0.   ]
 [0.    0.742 0.893 1.   ]]

## Q-learning
******************************Q-Learning converaged on episode 1327 **********************
Lake:
[['&' '.' '.' '.']
 ['.' '#' '.' '#']
 ['.' '.' '.' '#']
 ['#' '.' '.' '$']]
Policy:
[['→' '→' '↓' '←']
 ['↑' '↑' '↓' '↑']
 ['→' '↓' '↓' '↑']
 ['↑' '→' '→' '→']]
Value:
[[0.49  0.549 0.636 0.476]
 [0.422 0.    0.706 0.   ]
 [0.601 0.658 0.772 0.   ]
 [0.    0.774 0.856 1.   ]]



Question 5 : Try to find an optimal policy for the big frozen lake by tweaking the parameters for Sarsa control and Q-learning control (maximum number of episodes, learning rate, and exploration factor). You must use policy evaluation to confirm that the resulting policy is optimal.

## Sarsa
Lake:
[['&' '.' '.' '.' '.' '.' '.' '.']
 ['.' '.' '.' '.' '.' '.' '.' '.']
 ['.' '.' '.' '#' '.' '.' '.' '.']
 ['.' '.' '.' '.' '.' '#' '.' '.']
 ['.' '.' '.' '#' '.' '.' '.' '.']
 ['.' '#' '#' '.' '.' '.' '#' '.']
 ['.' '#' '.' '.' '#' '.' '#' '.']
 ['.' '.' '.' '#' '.' '.' '.' '$']]
Policy:
[['→' '→' '→' '→' '→' '→' '→' '↓']
 ['→' '→' '→' '→' '→' '→' '↓' '↓']
 ['↑' '↑' '↑' '↑' '→' '→' '↓' '↓']
 ['↑' '↑' '↑' '→' '↑' '↑' '→' '↓']
 ['↑' '↑' '↑' '↑' '→' '↓' '→' '↓']
 ['↑' '↑' '↑' '→' '→' '↓' '↑' '↓']                                                                                                                                                                                                                                                                       s\khalid\Desktop\QMUL\subjects\AI in Ga
 ['↑' '↑' '↓' '↑' '↑' '↓' '↑' '↓']
 ['↑' '←' '←' '↑' '→' '→' '→' '↑']]
Value:
[[0.184 0.207 0.232 0.262 0.295 0.331 0.373 0.417]
 [0.177 0.203 0.235 0.274 0.317 0.356 0.403 0.47 ]
 [0.134 0.157 0.192 0.    0.313 0.373 0.45  0.526]
 [0.077 0.098 0.115 0.142 0.201 0.    0.511 0.591]
 [0.029 0.032 0.04  0.    0.228 0.45  0.578 0.665]
 [0.011 0.    0.    0.138 0.212 0.538 0.    0.753]
 [0.002 0.    0.    0.006 0.    0.65  0.    0.87 ]
 [0.001 0.001 0.    0.    0.553 0.784 0.882 1.   ]]
Optimal value
[[0.18929821 0.21174597 0.23737683 0.26608964 0.29824139 0.33403301
  0.3740742  0.41801823]
 [0.20261399 0.2272996  0.25569216 0.28781496 0.33162148 0.37289195
  0.41890255 0.4694183 ]
 [0.20343361 0.22727901 0.25027192 0.         0.35567427 0.40792664
  0.47031099 0.52855937]
 [0.22616214 0.25460239 0.28757367 0.32518318 0.38283817 0.
  0.51918496 0.59522267]
 [0.20177241 0.22199537 0.24439998 0.         0.44146411 0.50665722
  0.58133736 0.67057767]
 [0.17589801 0.         0.         0.41711579 0.49147209 0.56715201
  0.         0.75557606]
 [0.1762132  0.         0.3005929  0.35401224 0.         0.6542872
  0.         0.86905418]
 [0.202151   0.22713442 0.26123183 0.         0.65629287 0.77060214
  0.86940005 1.        ]]

## Q-learning
******************************Q-Learning converaged on episode 29092 **********************
Lake:
[['&' '.' '.' '.' '.' '.' '.' '.']
 ['.' '.' '.' '.' '.' '.' '.' '.']
 ['.' '.' '.' '#' '.' '.' '.' '.']
 ['.' '.' '.' '.' '.' '#' '.' '.']
 ['.' '.' '.' '#' '.' '.' '.' '.']
 ['.' '#' '#' '.' '.' '.' '#' '.']
 ['.' '#' '.' '.' '#' '.' '#' '.']
 ['.' '.' '.' '#' '.' '.' '.' '$']]
Policy:
[['→' '→' '↓' '→' '↓' '↓' '→' '↓']
 ['↓' '↓' '→' '→' '→' '↓' '↓' '↓']
 ['→' '→' '↑' '↑' '→' '→' '→' '↓']
 ['→' '→' '→' '→' '↓' '↑' '→' '↓']
 ['→' '→' '↑' '↑' '→' '↓' '←' '↓']
 ['↑' '↑' '↑' '→' '→' '↓' '↑' '↓']
 ['↑' '↑' '→' '↑' '↑' '↓' '↑' '↓']
 ['→' '→' '↑' '↑' '↓' '→' '→' '↑']]
Value:
[[0.216 0.241 0.271 0.301 0.337 0.374 0.368 0.438]
 [0.206 0.238 0.302 0.336 0.377 0.417 0.46  0.506]
 [0.229 0.24  0.256 0.    0.417 0.462 0.509 0.557]
 [0.217 0.23  0.243 0.31  0.363 0.    0.564 0.618]
 [0.208 0.22  0.251 0.    0.525 0.589 0.522 0.727]
 [0.169 0.    0.    0.339 0.583 0.655 0.    0.81 ]
 [0.172 0.    0.32  0.424 0.    0.728 0.    0.9  ]
 [0.13  0.144 0.22  0.    0.689 0.81  0.9   1.   ]]
Optimal value
[[0.18929821 0.21174597 0.23737683 0.26608964 0.29824139 0.33403301
  0.3740742  0.41801823]
 [0.20261399 0.2272996  0.25569216 0.28781496 0.33162148 0.37289195
  0.41890255 0.4694183 ]
 [0.20343361 0.22727901 0.25027192 0.         0.35567427 0.40792664
  0.47031099 0.52855937]
 [0.22616214 0.25460239 0.28757367 0.32518318 0.38283817 0.
  0.51918496 0.59522267]
 [0.20177241 0.22199537 0.24439998 0.         0.44146411 0.50665722
  0.58133736 0.67057767]
 [0.17589801 0.         0.         0.41711579 0.49147209 0.56715201
  0.         0.75557606]
 [0.1762132  0.         0.3005929  0.35401224 0.         0.6542872
  0.         0.86905418]
 [0.202151   0.22713442 0.26123183 0.         0.65629287 0.77060214
  0.86940005 1.        ]]